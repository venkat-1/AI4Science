{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np    \n",
    "import csv\n",
    "import copy\n",
    "import random\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import mlpy\n",
    "#from mlpy import KernelRidge\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the DFT data + set of descriptors is read from a Data.csv file. Every data point is an impurity atom + defect site + CdX compound + DFT computed set of properties + complete set of descriptors. There are 3 DFT properties here: formation energy at Cd-rich, intermediate, and anion-rich chemical potential conditions. 1st set of descriptors is from column 7 to column 20, 2nd set of descriptors is from column 21 to column 25, and the complete set of descriptors is when columns 7 to 25 are used. Two further .csv files are read: Outside.csv which contains additional DFT computations performed on new compounds (CdTeSe and CdSeS alloys) and X.csv which contains descriptors for thousands of possible data points that make up the chemical space, about 25% of which were used to generate the DFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Read Data  ##\n",
    "\n",
    "ifile  = open('Data.csv', \"rt\")\n",
    "#ifile  = open('Data_norm.csv', \"rt\")\n",
    "reader = csv.reader(ifile)\n",
    "csvdata=[]\n",
    "for row in reader:\n",
    "        csvdata.append(row)   \n",
    "ifile.close()\n",
    "numrow=len(csvdata)\n",
    "numcol=len(csvdata[0]) \n",
    "csvdata = np.array(csvdata).reshape(numrow,numcol)\n",
    "dopant = csvdata[:,0]\n",
    "CdX = csvdata[:,1]\n",
    "doping_site = csvdata[:,2]\n",
    "prop  = csvdata[:,3]  ## Cd-rich Delta_H\n",
    "#prop  = csvdata[:,4]  ## Mod. Delta_H\n",
    "#prop  = csvdata[:,5]  ## X-rich Delta_H\n",
    "#X = csvdata[:,6:20]\n",
    "#X = csvdata[:,20:25]\n",
    "X = csvdata[:,6:]\n",
    "\n",
    "\n",
    "\n",
    "    # Read CdX alloy data: CdTe_0.5Se_0.5 and CdSe_0.5S_0.5\n",
    "ifile2  = open('Outside.csv', \"rt\")\n",
    "#ifile2  = open('Outside_norm.csv', \"rt\")\n",
    "reader2 = csv.reader(ifile2)\n",
    "csvdata2=[]\n",
    "for row2 in reader2:\n",
    "        csvdata2.append(row2)\n",
    "ifile2.close()\n",
    "numrow2=len(csvdata2)\n",
    "numcol2=len(csvdata2[0])\n",
    "csvdata2 = np.array(csvdata2).reshape(numrow2,numcol2)\n",
    "dopant_out = csvdata2[:,0]\n",
    "CdX_out = csvdata2[:,1]\n",
    "doping_site_out = csvdata2[:,2]\n",
    "prop_out  = csvdata2[:,3]\n",
    "#prop_out  = csvdata2[:,4]\n",
    "#prop_out  = csvdata2[:,5]\n",
    "#X_out = csvdata2[:,6:20]\n",
    "#X_out = csvdata2[:,20:25]\n",
    "X_out = csvdata2[:,6:]\n",
    "\n",
    "n_out = prop_out.size\n",
    "\n",
    "\n",
    "    # Read Entire Dataset                                                                                                              \n",
    "ifile3  = open('X.csv', \"rt\")\n",
    "#ifile3  = open('X_norm.csv', \"rt\")\n",
    "reader3 = csv.reader(ifile3)\n",
    "csvdata3=[]\n",
    "for row3 in reader3:\n",
    "        csvdata3.append(row3)\n",
    "ifile3.close()\n",
    "numrow3=len(csvdata3)\n",
    "numcol3=len(csvdata3[0])\n",
    "csvdata3 = np.array(csvdata3).reshape(numrow3,numcol3)\n",
    "dopant_all = csvdata3[:,0]\n",
    "CdX_all = csvdata3[:,1]\n",
    "doping_site_all = csvdata3[:,2]\n",
    "#X_all = csvdata3[:,3:17]\n",
    "#X_all = csvdata3[:,17:22]\n",
    "X_all = csvdata3[:,3:]\n",
    "\n",
    "n_all = dopant_all.size\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the dataset is divided into a training and test set. Applying t = 0.2 creates an 80-20 training-test split; change t for a different split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##  Train-Test Split  ##\n",
    "\n",
    "XX = copy.deepcopy(X)\n",
    "n = dopant.size\n",
    "m = np.int(X.size/n)\n",
    "\n",
    "t = 0.20\n",
    "\n",
    "X_train, X_test, Prop_train, Prop_test, dop_train, dop_test, sc_train, sc_test, ds_train, ds_test = train_test_split(XX, prop, dopant, CdX, doping_site, test_size=t)\n",
    "\n",
    "n_tr = Prop_train.size\n",
    "n_te = Prop_test.size\n",
    "\n",
    "\n",
    "Prop_train_fl = np.zeros(n_tr)\n",
    "for i in range(0,n_tr):\n",
    "    Prop_train_fl[i] = copy.deepcopy(float(Prop_train[i]))\n",
    "\n",
    "Prop_test_fl = np.zeros(n_te)\n",
    "for i in range(0,n_te):\n",
    "    Prop_test_fl[i] = copy.deepcopy(float(Prop_test[i]))\n",
    "    \n",
    "X_train_fl = [[0.0 for a in range(m)] for b in range(n_tr)]\n",
    "for i in range(0,n_tr):\n",
    "    for j in range(0,m):\n",
    "        X_train_fl[i][j] = np.float(X_train[i][j])\n",
    "\n",
    "X_test_fl = [[0.0 for a in range(m)] for b in range(n_te)]\n",
    "for i in range(0,n_te):\n",
    "    for j in range(0,m):\n",
    "        X_test_fl[i][j] = np.float(X_test[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML1: Define random forest regressor with default parameters which fits a model to the training data and makes predictions on the test dataset. Print out the training and test predictions as .csv or .txt files if desired.\n",
    "\n",
    "Comment if ML2 or ML3 are being used, un-comment if ML1 needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "###   Random Forest 1st Run   ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Train Random Forest Model  ##\n",
    "\n",
    "\n",
    "rfreg_opt = RandomForestRegressor(bootstrap=True, criterion='mae')\n",
    "\n",
    "rfreg_opt.fit(X_train,Prop_train)\n",
    "Pred_train = rfreg_opt.predict(X_train)\n",
    "Pred_test  = rfreg_opt.predict(X_test)\n",
    "Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "for i in range(0,Pred_train.size):\n",
    "    Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "for i in range(0,Pred_test.size):\n",
    "    Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "    \n",
    "#np.savetxt('Pred_train.csv', Pred_train_fl)\n",
    "#np.savetxt('Pred_test.csv', Pred_test_fl)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML2: Perform hyperparameter optimization when training random forest regression model. I define 5 hyperparameters to be tuned: number of estimators (number of trees in the forest), maximum depth of a tree, maximum features considered when looking for the best split, minimum number of samples required at a leaf node, and minimum number of samples required to split an internal node. All or some of these hyperparameters may be optimized by changing the rfregs_all definition loop. The random forest training loop runs over the length of rfregs_all and trains different RF models; the set of hyperparameters leading to minimum test error is taken as the optimal RF regressor, or rfreg_opt. Print out the training and test predictions as .csv or .txt files if desired.\n",
    "\n",
    "Comment if ML1 or ML3 are being used, un-comment if ML2 needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "###   Random Forest 2nd Run   ###\n",
    "\n",
    "\n",
    "\n",
    "  ##  Set of RF Model Parameters  ##\n",
    "\n",
    "    \n",
    "\n",
    "rfregs_all = list()\n",
    "\n",
    "n_est_all = [50, 100, 200]\n",
    "max_depth_all = [40, 70, 100]\n",
    "max_feat_all = [10, 15, m]\n",
    "min_samp_leaf_all = [1, 3, 5]\n",
    "min_samp_split_all = [2, 5, 10]\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        for k in range(0,3):\n",
    "            for l in range(0,3):\n",
    "#                for h in range(0,3):\n",
    "#        rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], max_features=15, min_samples_leaf=5, min_samples_split=10)\n",
    "#        rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=100, max_depth=70, min_samples_leaf=min_samp_leaf_all[i], min_samples_split=min_samp_split_all[j], max_features=15)\n",
    "#                    rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], min_samples_leaf=min_samp_leaf_all[k], min_samples_split=min_samp_split_all[l], max_features=max_feat_all[h])\n",
    "                rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], min_samples_leaf=min_samp_leaf_all[k], min_samples_split=min_samp_split_all[l])\n",
    "                rfregs_all.append(rfreg_temp)\n",
    "\n",
    "\n",
    "\n",
    "##  Train Random Forest Model  ##\n",
    "\n",
    "\n",
    "times = len(rfregs_all)\n",
    "\n",
    "train_errors = [0.0]*times\n",
    "test_errors = [0.0]*times\n",
    "\n",
    "for k in range(0,times):\n",
    "    rfreg = rfregs_all[k]\n",
    "    rfreg.fit(X_train,Prop_train)\n",
    "    Pred_train = rfreg.predict(X_train)\n",
    "    Pred_test  = rfreg.predict(X_test)\n",
    "    Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "    Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "    for i in range(0,Pred_train.size):\n",
    "        Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "    for i in range(0,Pred_test.size):\n",
    "        Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "    train_errors[k] = sklearn.metrics.mean_squared_error(Prop_train_fl, Pred_train_fl)\n",
    "    test_errors[k] = sklearn.metrics.mean_squared_error(Prop_test_fl, Pred_test_fl)\n",
    "\n",
    "i_opt = np.argmin(test_errors)\n",
    "rfreg_opt = rfregs_all[i_opt]\n",
    "\n",
    "\n",
    "rfreg_opt.fit(X_train,Prop_train)\n",
    "Pred_train = rfreg_opt.predict(X_train)\n",
    "Pred_test  = rfreg_opt.predict(X_test)\n",
    "Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "for i in range(0,Pred_train.size):\n",
    "    Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "for i in range(0,Pred_test.size):\n",
    "    Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML3: Perform hyperparameter optimization (same as ML2) + cross-validation (used to take care of overfitting in machine learning). Change n-fold to define the type of cross-validation (CV), for eg. 5-fold CV. The RF training loop once again runs over the length of rfregs_all but at every point, divides the training set into n folds, uses (n-1) sets to train the model and tests on the n-th set. The CV training and CV test errors are defined as averages over the n sets; the set of hyperparameters leading to minimum CV test error is taken as the optimal regressor rfreg_opt. Print out the training and test predictions as .csv or .txt files if desired.\n",
    "\n",
    "Comment out if ML1 or ML2 are being used, un-comment if ML3 needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###   Random Forest 3rd Run   ###\n",
    "\n",
    "\n",
    "  ##  Set of RF Model Parameters  ##\n",
    "\n",
    "\n",
    "rfregs_all = list()\n",
    "\n",
    "#n_est_all = [50, 100, 200, 300, 500]\n",
    "#max_depth_all = [30, 75, 100, 150, 200]\n",
    "\n",
    "n_est_all = [50, 100, 200]\n",
    "max_depth_all = [40, 70, 100]\n",
    "max_feat_all = [5, 10, m]\n",
    "min_samp_leaf_all = [1, 3, 5]\n",
    "min_samp_split_all = [2, 5, 10]\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "#        for k in range(0,3):\n",
    "#            for l in range(0,3):\n",
    "#                for h in range(0,3):\n",
    "#        rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], max_features=15, min_samples_leaf=5, min_samples_split=10)\n",
    "        rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=100, max_depth=70, min_samples_leaf=min_samp_leaf_all[i], min_samples_split=min_samp_split_all[j], max_features=m)\n",
    "#                    rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], min_samples_leaf=min_samp_leaf_all[k], min_samples_split=min_samp_split_all[l], max_features=max_feat_all[h])\n",
    "#                rfreg_temp = RandomForestRegressor(bootstrap=True, criterion='mae', n_estimators=n_est_all[i], max_depth=max_depth_all[j], min_samples_leaf=min_samp_leaf_all[k], min_samples_split=min_samp_split_all[l])\n",
    "        rfregs_all.append(rfreg_temp)\n",
    "\n",
    "\n",
    "\n",
    "##  Train Random Forest Model  ##\n",
    "\n",
    "\n",
    "times = len(rfregs_all)\n",
    "\n",
    "train_errors = [0.0]*times\n",
    "test_errors = [0.0]*times\n",
    "\n",
    "\n",
    "##  Cross-validation  ##\n",
    "\n",
    "n_fold = 5\n",
    "\n",
    "for i in range(0,times):\n",
    "    kf = KFold(n_splits = n_fold)\n",
    "    mse_test_cv = 0.00\n",
    "    mse_train_cv = 0.00\n",
    "    count = 0\n",
    "    rfreg = rfregs_all[i]\n",
    "    for train, test in kf.split(X_train):\n",
    "        X_train_cv, X_test_cv, Prop_train_cv, Prop_test_cv = X_train[train], X_train[test], Prop_train[train], Prop_train[test]\n",
    "\n",
    "        rfreg.fit(X_train_cv,Prop_train_cv)\n",
    "        Prop_pred_train_cv = rfreg.predict(X_train_cv)\n",
    "        Prop_pred_test_cv  = rfreg.predict(X_test_cv)\n",
    "\n",
    "        n_tr_cv = Prop_train_cv.size\n",
    "        n_te_cv = Prop_test_cv.size\n",
    "        Prop_test_cv_fl = [0.0]*n_te_cv\n",
    "        Prop_pred_test_cv_fl = [0.0]*n_te_cv\n",
    "        Prop_train_cv_fl = [0.0]*n_tr_cv\n",
    "        Prop_pred_train_cv_fl = [0.0]*n_tr_cv\n",
    "\n",
    "        for a in range(0,n_tr_cv):\n",
    "            Prop_train_cv_fl[a] = np.float(Prop_train_cv[a])\n",
    "            Prop_pred_train_cv_fl[a] = np.float(Prop_pred_train_cv[a])\n",
    "        for a in range(0,n_te_cv):\n",
    "            Prop_test_cv_fl[a] = np.float(Prop_test_cv[a])\n",
    "            Prop_pred_test_cv_fl[a] = np.float(Prop_pred_test_cv[a])\n",
    "\n",
    "        mse_test_cv = mse_test_cv  + sklearn.metrics.mean_squared_error(Prop_test_cv_fl,Prop_pred_test_cv_fl)\n",
    "        mse_train_cv = mse_train_cv + sklearn.metrics.mean_squared_error(Prop_train_cv_fl,Prop_pred_train_cv_fl)\n",
    "    mse_test = mse_test_cv / n_fold\n",
    "    mse_train = mse_train_cv / n_fold\n",
    "    train_errors[i] = mse_train\n",
    "    test_errors[i] = mse_test\n",
    "i_opt = np.argmin(test_errors)\n",
    "rfreg_opt = rfregs_all[i_opt]\n",
    "\n",
    "\n",
    "rfreg_opt.fit(X_train,Prop_train)\n",
    "Pred_train = rfreg_opt.predict(X_train)\n",
    "Pred_test  = rfreg_opt.predict(X_test)\n",
    "Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "for i in range(0,Pred_train.size):\n",
    "    Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "for i in range(0,Pred_test.size):\n",
    "    Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Ridge Regression with hyperparameter optimization using GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mannodiarun/gsas2full/lib/python3.7/site-packages/sklearn/utils/validation.py:755: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  estimator=estimator)\n",
      "/Users/mannodiarun/gsas2full/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/Users/mannodiarun/gsas2full/lib/python3.7/site-packages/sklearn/metrics/_regression.py:85: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n",
      "/Users/mannodiarun/gsas2full/lib/python3.7/site-packages/sklearn/metrics/_regression.py:85: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7bc9fe6aafd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#krr_opt = KernelRidge(alpha=1, kernel='linear', gamma=0.2, degree=3, coef0=1, kernel_params=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mkrr_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mProp_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mPred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkrr_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mPred_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mkrr_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                       *args, **kwargs)\n\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    434\u001b[0m                           \"multioutput='uniform_average').\", FutureWarning)\n\u001b[1;32m    435\u001b[0m         return r2_score(y, y_pred, sample_weight=sample_weight,\n\u001b[0;32m--> 436\u001b[0;31m                         multioutput='variance_weighted')\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gsas2full/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n\u001b[0m\u001b[1;32m    592\u001b[0m                                                       dtype=np.float64)\n\u001b[1;32m    593\u001b[0m     denominator = (weight * (y_true - np.average(\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "##  Train Kernel Ridge Regression Model  ##\n",
    "\n",
    "\n",
    "param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n",
    "              \"kernel\": [ExpSineSquared(l, p)\n",
    "                         for l in np.logspace(-2, 2, 10)\n",
    "                         for p in np.logspace(0, 2, 10)]}\n",
    "krr_opt = GridSearchCV(KernelRidge(), param_grid=param_grid)\n",
    "\n",
    "#krr_opt = KernelRidge(alpha=1, kernel='linear', gamma=0.2, degree=3, coef0=1, kernel_params=None)\n",
    "\n",
    "krr_opt.fit(X_train,Prop_train)\n",
    "Pred_train = krr_opt.predict(X_train)\n",
    "Pred_test  = krr_opt.predict(X_test)\n",
    "Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "for i in range(0,Pred_train.size):\n",
    "    Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "for i in range(0,Pred_test.size):\n",
    "    Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "    \n",
    "#np.savetxt('Pred_train.csv', Pred_train_fl)\n",
    "#np.savetxt('Pred_test.csv', Pred_test_fl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest Regression with hyperparameter optimization using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [50, 100, 200],\n",
    "              \"max_depth\": [40, 70, 100],\n",
    "              \"max_features\": [10, 15, m],\n",
    "              \"min_samples_leaf\": [1, 3, 5],\n",
    "              \"min_samples_split\": [2, 5, 10]}\n",
    "\n",
    "rfregs_opt = GridSearchCV(RandomForestRegressor(), param_grid=param_grid)\n",
    "\n",
    "rfreg_opt.fit(X_train,Prop_train)\n",
    "Pred_train = rfreg_opt.predict(X_train)\n",
    "Pred_test  = rfreg_opt.predict(X_test)\n",
    "Pred_train_fl = [0.0]*(Pred_train.size)\n",
    "Pred_test_fl = [0.0]*(Pred_test.size)\n",
    "for i in range(0,Pred_train.size):\n",
    "    Pred_train_fl[i] = np.float(Pred_train[i])\n",
    "for i in range(0,Pred_test.size):\n",
    "    Pred_test_fl[i] = np.float(Pred_test[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimal RF model, rfreg_opt, as trained using ML1, ML2 or ML3, to make predictions on (a) the outside dataset, which contains additional DFT data generated for CdTeSe and CdSeS compounds (which were not included at all in Data.csv, the file used for training and testing the regression models), and (b) the entire chemical space, which contains all the DFT data points + the missing data points which make up ~ 75% of the total data points. Print out the predictions as .csv or .txt files if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###    Outside Prediction    ###\n",
    "\n",
    "\n",
    "Pred_out = rfreg_opt.predict(X_out)\n",
    "#Pred_out = krr_opt.predict(X_out)\n",
    "Pred_out_fl = [0.0]*Pred_out.size\n",
    "Prop_out_fl = [0.0]*Pred_out.size\n",
    "for i in range(0,prop_out.size):\n",
    "    Prop_out_fl[i] = np.float(prop_out[i])\n",
    "for i in range(0,prop_out.size):\n",
    "    Pred_out_fl[i] = np.float(Pred_out[i])\n",
    "\n",
    "#np.savetxt('Pred_out.csv', Pred_out_fl)\n",
    "\n",
    "\n",
    "\n",
    "Pred_all = rfreg_opt.predict(X_all)\n",
    "#Pred_all = krr_opt.predict(X_all)\n",
    "Pred_all_fl = [0.0]*Pred_all.size\n",
    "for i in range(0,Pred_all.size):\n",
    "    Pred_all_fl[i] = np.float(Pred_all[i])\n",
    "\n",
    "#np.savetxt('Pred_all.csv', Pred_all_fl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If error bars are desired, they can be calculated based on the standard deviation of predictions from across the different estimators or trees in the forest. Change the percentile from 95 if needed. Error bars are estimated based on the RF predictors for the training, test, outside and entire datasets.\n",
    "\n",
    "Comment out this section if errors are not desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "\n",
    "##  Error Bars of Training and Test Datasets  ##\n",
    "\n",
    "\n",
    "percentile = 95\n",
    "\n",
    "\n",
    "err_up_train   = [0.0]*n_tr\n",
    "err_down_train = [0.0]*n_tr\n",
    "preds_fl = [[0.0 for a in range(n_tr)] for b in range (len(rfreg_opt.estimators_))]\n",
    "z = 0\n",
    "\n",
    "for pred in rfreg_opt.estimators_:\n",
    "    preds = pred.predict(X_train)\n",
    "    for i in range(0,n_tr):\n",
    "        preds_fl[z][i] = np.float(preds[i])\n",
    "    z = z+1\n",
    "\n",
    "pp = [0.0]*len(rfreg_opt.estimators_)\n",
    "for i in range(n_tr):\n",
    "    for j in range(0,len(rfreg_opt.estimators_)):\n",
    "        pp[j] = preds_fl[j][i]\n",
    "    err_down_train[i] = np.percentile(pp[:], (100 - percentile) / 2. )\n",
    "    err_up_train[i] = np.percentile(pp[:], 100 - (100 - percentile) / 2.)\n",
    "\n",
    "\n",
    "\n",
    "err_up_test   = [0.0]*n_te\n",
    "err_down_test = [0.0]*n_te\n",
    "preds_fl = [[0.0 for a in range(n_te)] for b in range (len(rfreg_opt.estimators_))]\n",
    "z = 0\n",
    "\n",
    "for pred in rfreg_opt.estimators_:\n",
    "    preds = pred.predict(X_test)\n",
    "    for i in range(0,n_te):\n",
    "        preds_fl[z][i] = np.float(preds[i])\n",
    "    z = z+1\n",
    "\n",
    "pp = [0.0]*len(rfreg_opt.estimators_)\n",
    "for i in range(n_te):\n",
    "    for j in range(0,len(rfreg_opt.estimators_)):\n",
    "        pp[j] = preds_fl[j][i]\n",
    "    err_down_test[i] = np.percentile(pp[:], (100 - percentile) / 2. )\n",
    "    err_up_test[i] = np.percentile(pp[:], 100 - (100 - percentile) / 2.)\n",
    "\n",
    "\n",
    "#np.savetxt('up_train.csv', err_up_train)\n",
    "#np.savetxt('down_train.csv', err_down_train)\n",
    "#np.savetxt('up_test.csv', err_up_test)\n",
    "#np.savetxt('down_test.csv', err_down_test)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###     Error Bars on Outside Dataset    ###\n",
    "    \n",
    "    \n",
    "    \n",
    "err_up_out = [0.0]*n_out\n",
    "err_down_out = [0.0]*n_out\n",
    "preds_fl = [[0.0 for a in range(n_out)] for b in range (len(rfreg_opt.estimators_))]\n",
    "z = 0\n",
    "\n",
    "for pred in rfreg_opt.estimators_:\n",
    "    preds = pred.predict(X_out)\n",
    "    for i in range(0,n_out):\n",
    "        preds_fl[z][i] = np.float(preds[i])\n",
    "    z = z+1\n",
    "\n",
    "pp = [0.0]*len(rfreg_opt.estimators_)\n",
    "for i in range(n_out):\n",
    "    for j in range(0,len(rfreg_opt.estimators_)):\n",
    "        pp[j] = preds_fl[j][i]\n",
    "    err_down_out[i] = np.percentile(pp[:], (100 - percentile) / 2. )\n",
    "    err_up_out[i] = np.percentile(pp[:], 100 - (100 - percentile) / 2.)\n",
    "\n",
    "\n",
    "up_out = [0.0]*n_out\n",
    "down_out = [0.0]*n_out\n",
    "\n",
    "for i in range(0,n_out):\n",
    "    up_out[i] = err_up_out[i] - Pred_out_fl[i]\n",
    "    down_out[i] = Pred_out_fl[i] - err_down_out[i]\n",
    "\n",
    "\n",
    "#np.savetxt('up_out.csv', up_out)\n",
    "#np.savetxt('down_out.csv', down_out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "###     Error Bars for Entire Chemical Space    ###\n",
    "\n",
    "\n",
    "err_up_all = [0.0]*n_all\n",
    "err_down_all = [0.0]*n_all\n",
    "preds_fl = [[0.0 for a in range(n_all)] for b in range (len(rfreg_opt.estimators_))]\n",
    "z = 0\n",
    "\n",
    "for pred in rfreg_opt.estimators_:\n",
    "    preds = pred.predict(X_all)\n",
    "    for i in range(0,n_all):\n",
    "        preds_fl[z][i] = np.float(preds[i])\n",
    "    z = z+1\n",
    "\n",
    "pp = [0.0]*len(rfreg_opt.estimators_)\n",
    "for i in range(n_all):\n",
    "    for j in range(0,len(rfreg_opt.estimators_)):\n",
    "        pp[j] = preds_fl[j][i]\n",
    "    err_down_all[i] = np.percentile(pp[:], (100 - percentile) / 2. )\n",
    "    err_up_all[i] = np.percentile(pp[:], 100 - (100 - percentile) / 2.)\n",
    "\n",
    "\n",
    "up_all = [0.0]*n_all\n",
    "down_all = [0.0]*n_all\n",
    "\n",
    "for i in range(0,n_all):\n",
    "    up_all[i] = err_up_all[i] - Pred_all_fl[i]\n",
    "    down_all[i] = Pred_all_fl[i] - err_down_all[i]\n",
    "\n",
    "\n",
    "\n",
    "#np.savetxt('up_all.csv', up_all)\n",
    "#np.savetxt('down_all.csv', down_all)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide actual and predicted data by CdX compound type (CdTe or CdSe or CdS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##  Predicted Data by Type of CdX Compound  ##\n",
    "\n",
    "\n",
    "Prop_train_CdTe = [0.0]*len(Prop_train_fl)\n",
    "err_up_train_CdTe = copy.deepcopy(err_up_train)\n",
    "err_down_train_CdTe = copy.deepcopy(err_down_train)\n",
    "\n",
    "Prop_train_CdSe = [0.0]*len(Prop_train_fl)\n",
    "err_up_train_CdSe = copy.deepcopy(err_up_train)\n",
    "err_down_train_CdSe = copy.deepcopy(err_down_train)\n",
    "\n",
    "Prop_train_CdS = [0.0]*len(Prop_train_fl)\n",
    "err_up_train_CdS = copy.deepcopy(err_up_train)\n",
    "err_down_train_CdS = copy.deepcopy(err_down_train)\n",
    "\n",
    "Prop_test_CdTe = [0.0]*len(Prop_test_fl)\n",
    "err_up_test_CdTe = copy.deepcopy(err_up_test)\n",
    "err_down_test_CdTe = copy.deepcopy(err_down_test)\n",
    "\n",
    "Prop_test_CdSe = [0.0]*len(Prop_test_fl)\n",
    "err_up_test_CdSe = copy.deepcopy(err_up_test)\n",
    "err_down_test_CdSe = copy.deepcopy(err_down_test)\n",
    "\n",
    "Prop_test_CdS = [0.0]*len(Prop_test_fl)\n",
    "err_up_test_CdS = copy.deepcopy(err_up_test)\n",
    "err_down_test_CdS = copy.deepcopy(err_down_test)\n",
    "\n",
    "\n",
    "Pred_train_CdTe = [0.0]*len(Pred_train_fl)\n",
    "Pred_train_CdSe = [0.0]*len(Pred_train_fl)\n",
    "Pred_train_CdS = [0.0]*len(Pred_train_fl)\n",
    "Pred_test_CdTe = [0.0]*len(Pred_test_fl)\n",
    "Pred_test_CdSe = [0.0]*len(Pred_test_fl)\n",
    "Pred_test_CdS = [0.0]*len(Pred_test_fl)\n",
    "\n",
    "\n",
    "\n",
    "aa = 0\n",
    "bb = 0\n",
    "cc = 0\n",
    "dd = 0\n",
    "ee = 0\n",
    "ff = 0\n",
    "gg = 0\n",
    "hh = 0\n",
    "ii = 0\n",
    "jj = 0\n",
    "kk = 0\n",
    "ll = 0\n",
    "\n",
    "for i in range(0,Prop_train_fl.size):\n",
    "    if sc_train[i] == 'CdTe':\n",
    "        Prop_train_CdTe[aa] = Prop_train_fl[i]\n",
    "        Pred_train_CdTe[aa] = Pred_train_fl[i]\n",
    "        err_up_train_CdTe[aa] = err_up_train[i]\n",
    "        err_down_train_CdTe[aa] = err_down_train[i]\n",
    "        aa = aa+1\n",
    "    if sc_train[i] == 'CdSe':\n",
    "        Prop_train_CdSe[bb] = Prop_train_fl[i]\n",
    "        Pred_train_CdSe[bb] = Pred_train_fl[i]\n",
    "        err_up_train_CdSe[bb] = err_up_train[i]\n",
    "        err_down_train_CdSe[bb] = err_down_train[i]\n",
    "        bb = bb+1\n",
    "    if sc_train[i] == 'CdS':\n",
    "        Prop_train_CdS[cc] = Prop_train_fl[i]\n",
    "        Pred_train_CdS[cc] = Pred_train_fl[i]\n",
    "        err_up_train_CdS[cc] = err_up_train[i]\n",
    "        err_down_train_CdS[cc] = err_down_train[i]\n",
    "        cc = cc+1\n",
    "\n",
    "for i in range(0,Prop_test_fl.size):\n",
    "    if sc_test[i] == 'CdTe':\n",
    "        Prop_test_CdTe[dd] = Prop_test_fl[i]\n",
    "        Pred_test_CdTe[dd] = Pred_test_fl[i]\n",
    "        err_up_test_CdTe[dd] = err_up_test[i]\n",
    "        err_down_test_CdTe[dd] = err_down_test[i]\n",
    "        dd = dd+1\n",
    "    if sc_test[i] == 'CdSe':\n",
    "        Prop_test_CdSe[ee] = Prop_test_fl[i]\n",
    "        Pred_test_CdSe[ee] = Pred_test_fl[i]\n",
    "        err_up_test_CdSe[ee] = err_up_test[i]\n",
    "        err_down_test_CdSe[ee] = err_down_test[i]\n",
    "        ee = ee+1\n",
    "    if sc_test[i] == 'CdS':\n",
    "        Prop_test_CdS[ff] = Prop_test_fl[i]\n",
    "        Pred_test_CdS[ff] = Pred_test_fl[i]\n",
    "        err_up_test_CdS[ff] = err_up_test[i]\n",
    "        err_down_test_CdS[ff] = err_down_test[i]\n",
    "        ff = ff+1\n",
    "\n",
    "\n",
    "\n",
    "up_train_CdTe = [0.0]*aa\n",
    "down_train_CdTe = [0.0]*aa\n",
    "up_test_CdTe = [0.0]*dd\n",
    "down_test_CdTe = [0.0]*dd\n",
    "\n",
    "up_train_CdSe = [0.0]*bb\n",
    "down_train_CdSe = [0.0]*bb\n",
    "up_test_CdSe = [0.0]*ee\n",
    "down_test_CdSe = [0.0]*ee\n",
    "\n",
    "up_train_CdS = [0.0]*cc\n",
    "down_train_CdS = [0.0]*cc\n",
    "up_test_CdS = [0.0]*ff\n",
    "down_test_CdS = [0.0]*ff\n",
    "\n",
    "\n",
    "for i in range(0,aa):\n",
    "    up_train_CdTe[i]   = err_up_train_CdTe[i] - Pred_train_CdTe[i]\n",
    "    down_train_CdTe[i] = Pred_train_CdTe[i] - err_down_train_CdTe[i]\n",
    "for i in range(0,bb):\n",
    "    up_train_CdSe[i] = err_up_train_CdSe[i] - Pred_train_CdSe[i]\n",
    "    down_train_CdSe[i] = Pred_train_CdSe[i] - err_down_train_CdSe[i]\n",
    "for i in range(0,cc):\n",
    "    up_train_CdS[i] = err_up_train_CdS[i] - Pred_train_CdS[i]\n",
    "    down_train_CdS[i] = Pred_train_CdS[i] - err_down_train_CdS[i]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,dd):\n",
    "    up_test_CdTe[i] = err_up_test_CdTe[i] - Pred_test_CdTe[i]\n",
    "    down_test_CdTe[i] = Pred_test_CdTe[i] - err_down_test_CdTe[i]\n",
    "for i in range(0,ee):\n",
    "    up_test_CdSe[i] = err_up_test_CdSe[i] - Pred_test_CdSe[i]\n",
    "    down_test_CdSe[i] = Pred_test_CdSe[i] - err_down_test_CdSe[i]\n",
    "for i in range(0,ff):\n",
    "    up_test_CdS[i] = err_up_test_CdS[i] - Pred_test_CdS[i]\n",
    "    down_test_CdS[i] = Pred_test_CdS[i] - err_down_test_CdS[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out prediction root mean square errors for training, test and outside points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Calculate Prediction RMSE  ##\n",
    "\n",
    "\n",
    "rmse_test_prop = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_test_fl, Pred_test_fl) )\n",
    "rmse_train_prop = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_train_fl, Pred_train_fl) )\n",
    "print('rmse_test_prop=', rmse_test_prop)\n",
    "print('rmse_train_prop=', rmse_train_prop)\n",
    "\n",
    "rmse_test_CdTe = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_test_CdTe[0:dd], Pred_test_CdTe[0:dd]) )\n",
    "rmse_train_CdTe = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_train_CdTe[0:aa], Pred_train_CdTe[0:aa]) )\n",
    "print('rmse_test_CdTe=', rmse_test_CdTe)\n",
    "print('rmse_train_CdTe=', rmse_train_CdTe)\n",
    "\n",
    "rmse_test_CdSe = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_test_CdSe[0:ee], Pred_test_CdSe[0:ee]) )\n",
    "rmse_train_CdSe = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_train_CdSe[0:bb], Pred_train_CdSe[0:bb]) )\n",
    "print('rmse_test_CdSe=', rmse_test_CdSe)\n",
    "print('rmse_train_CdSe=', rmse_train_CdSe)\n",
    "\n",
    "rmse_test_CdS = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_test_CdS[0:ff], Pred_test_CdS[0:ff]) )\n",
    "rmse_train_CdS = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_train_CdS[0:cc], Pred_train_CdS[0:cc]) )\n",
    "print('rmse_test_CdS=', rmse_test_CdS)\n",
    "print('rmse_train_CdS=', rmse_train_CdS)\n",
    "\n",
    "rmse_CdTeSe = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_out_fl[0:22], Pred_out_fl[0:22]) )\n",
    "rmse_CdSeS = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_out_fl[22:44], Pred_out_fl[22:44]) )\n",
    "print('rmse_test_CdTeSe=', rmse_CdTeSe)\n",
    "print('rmse_train_CdSeS=', rmse_CdSeS)\n",
    "rmse_out = np.sqrt ( sklearn.metrics.mean_squared_error(Prop_out_fl[0:44], Pred_out_fl[0:44]) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot regression results as DFT vs ML parity plots; comment out error bars if not desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Plot Regression Results  ##\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.subplots_adjust(left=0.16, bottom=0.16, right=0.95, top=0.90)\n",
    "plt.rc('font', family='Arial narrow')\n",
    "\n",
    "plt.title('RFR Model, Cd-rich Formation Energy (eV)', fontsize=20, pad=12)\n",
    "#plt.title('KRR Model, Cd-rich Formation Energy (eV)', fontsize=20, pad=12)\n",
    "\n",
    "a = [-175,0,125]\n",
    "b = [-175,0,125]\n",
    "plt.plot(b, a, c='k', ls='-')\n",
    "\n",
    "plt.ylabel('ML Prediction', fontname='Arial Narrow', size=32)\n",
    "plt.xlabel('DFT Calculation', fontname='Arial Narrow', size=32)\n",
    "plt.rc('xtick', labelsize=28)\n",
    "plt.rc('ytick', labelsize=28)\n",
    "plt.ylim([-1.0, 9.3])\n",
    "plt.xlim([-1.0, 9.3])\n",
    "\n",
    "#plt.scatter(Prop_train_CdTe[:], Pred_train_CdTe[:], c='blue', marker='s', s=100, edgecolors='dimgrey', alpha=1.0, label='Training')\n",
    "#plt.scatter(Prop_train_CdSe[:], Pred_train_CdSe[:], c='blue',marker='^', s=150, edgecolors='dimgrey', alpha=1.0, label='_nolegend_')\n",
    "#plt.scatter(Prop_train_CdS[:], Pred_train_CdS[:], c='blue', marker='*', s=200, edgecolors='dimgrey', alpha=1.0, label='_nolegend_')\n",
    "#plt.scatter(Prop_test_CdTe[:], Pred_test_CdTe[:], c='orange', marker='s', s=100, edgecolors='dimgrey', alpha=0.2, label='Test')\n",
    "#plt.scatter(Prop_test_CdSe[:], Pred_test_CdSe[:], c='orange', marker='^', s=150, edgecolors='dimgrey', alpha=0.2, label='_nolegend_')\n",
    "#plt.scatter(Prop_test_CdS[:], Pred_test_CdS[:], c='orange', marker='*', s=200, edgecolors='dimgrey', alpha=0.2, label='_nolegend_')\n",
    "\n",
    "plt.scatter(Prop_train_fl[:], Pred_train_fl[:], c='blue', marker='*', s=200, edgecolors='dimgrey', alpha=1.0, label='Training')\n",
    "plt.scatter(Prop_test_fl[:], Pred_test_fl[:], c='orange', marker='*', s=200, edgecolors='dimgrey', alpha=0.2, label='Test')\n",
    "\n",
    "plt.scatter(Prop_out_fl[0:22], Pred_out_fl[0:22], c='red', marker='h', s=200, edgecolors='dimgrey', alpha=0.2, label='CdTe$_{0.5}$Se$_{0.5}$')\n",
    "plt.scatter(Prop_out_fl[22:44], Pred_out_fl[22:44], c='green', marker='h', s=200, edgecolors='dimgrey', alpha=0.2, label='CdSe$_{0.5}$S$_{0.5}$')\n",
    "\n",
    "# plt.errorbar(Prop_train_CdTe[0:aa], Pred_train_CdTe[0:aa], yerr = [up_train_CdTe[0:aa], down_train_CdTe[0:aa]], c='blue', marker='s', alpha=1.0, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='blue', capthick=1, label='Training')\n",
    "# plt.errorbar(Prop_train_CdSe[0:bb], Pred_train_CdSe[0:bb], yerr = [up_train_CdSe[0:bb], down_train_CdSe[0:bb]], c='blue', marker='^', alpha=1.0, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='blue', capthick=1, label='_nolegend_')\n",
    "# plt.errorbar(Prop_train_CdS[0:cc], Pred_train_CdS[0:cc], yerr = [up_train_CdS[0:cc], down_train_CdS[0:cc]], c='blue', marker='*', alpha=1.0, markeredgecolor='dimgrey', markersize=12, fmt='o', ecolor='blue', capthick=1, label='_nolegend_')\n",
    "\n",
    "# plt.errorbar(Prop_test_CdTe[0:dd], Pred_test_CdTe[0:dd], yerr = [up_test_CdTe[0:dd], down_test_CdTe[0:dd]], c='orange', marker='s', alpha=0.2, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='orange', capthick=1, label='Test')\n",
    "# plt.errorbar(Prop_test_CdSe[0:ee], Pred_test_CdSe[0:ee], yerr = [up_test_CdSe[0:ee], down_test_CdSe[0:ee]], c='orange', marker='^', alpha=0.2, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='orange', capthick=1, label='_nolegend_')\n",
    "# plt.errorbar(Prop_test_CdS[0:ff], Pred_test_CdS[0:ff], yerr = [up_test_CdS[0:ff], down_test_CdS[0:ff]], c='orange', marker='*', alpha=0.2, markeredgecolor='dimgrey', markersize=12, fmt='o', ecolor='orange', capthick=1, label='_nolegend_')\n",
    "\n",
    "# plt.errorbar(Prop_out_fl[0:22], Pred_out_fl[0:22], yerr = [up_out[0:22], down_out[0:22]], c='red', marker='h', alpha=0.2, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='red', capthick=1, label='CdTe$_{0.5}$Se$_{0.5}$')\n",
    "# plt.errorbar(Prop_out_fl[22:44], Pred_out_fl[22:44], yerr = [up_out[22:44], down_out[22:44]], c='green', marker='h', alpha=0.2, markeredgecolor='dimgrey', markersize=8, fmt='o', ecolor='green', capthick=1, label='CdSe$_{0.5}$S$_{0.5}$')\n",
    "\n",
    "\n",
    "te = '%.2f' % rmse_test_prop\n",
    "tr = '%.2f' % rmse_train_prop\n",
    "out = '%.2f' % rmse_out\n",
    "\n",
    "plt.text(5.2, 1.5, 'Test_rmse = ', c='r', fontsize=16)\n",
    "plt.text(7.6, 1.5, te, c='r', fontsize=16)\n",
    "plt.text(8.5, 1.5, 'eV', c='r', fontsize=16)\n",
    "plt.text(5.1, 0.8, 'Train_rmse = ', c='r', fontsize=16)\n",
    "plt.text(7.6, 0.8, tr, c='r', fontsize=16)\n",
    "plt.text(8.5, 0.8, 'eV', c='r', fontsize=16)\n",
    "plt.text(5.4, 0.1, 'Out_rmse = ', c='r', fontsize=16)\n",
    "plt.text(7.6, 0.1, out, c='r', fontsize=16)\n",
    "plt.text(8.5, 0.1, 'eV', c='r', fontsize=16)\n",
    "\n",
    "plt.xticks([0, 2, 4, 6, 8])\n",
    "plt.yticks([0, 2, 4, 6, 8])\n",
    "plt.legend(loc='upper left',ncol=1, frameon=True, prop={'family':'Arial narrow','size':16})\n",
    "plt.savefig('plot_Cd_rich.pdf', dpi=450)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
